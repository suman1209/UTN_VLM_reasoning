{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114c902a-652d-4fcb-a8cd-a46224d2b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unsloth import FastModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626d6624-b89c-404b-8d0f-7a92faea12f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "\n",
    "    # Other popular models!\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/Llama-3.3-70B\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
    "    \"unsloth/Phi-4\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length = 2048, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d8aac0-49b9-48d1-a7b8-a0527f27d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b9e164-a412-47f2-aa7b-4bc81ecb85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb38f262-c092-4685-a628-c5c4a4d12fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "## Env variables and preparation stuffs\n",
    "sys.path.insert(0, \"../\")\n",
    "from src_code.data_utils.dataset import GridDataset\n",
    "from src_code.data_utils.dataset_utils import dataset_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf5f43f-7f48-44d9-ba2d-67034bbc210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_role = \"You are a helpful assistant working in path planning in a 2D grid.\"\n",
    "dataset = GridDataset(grid_size=5, seed = 42, wall_symbol=\"#\", free_symbol=\".\", cell_size=10)\n",
    "ft_dataset = dataset_generator(dataset, sys_role, train_num=100000, val_num=2, test_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2103f38-3d9e-4f01-b71a-473e598ea8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_data_formats\n",
    "ft_dataset = standardize_data_formats(ft_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4baa5b5-66d4-4947-b40c-9b56fc075fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4e5ff5-ccad-418d-a421-e9f977b7fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(examples):\n",
    "    texts = tokenizer.apply_chat_template(examples[\"messages\"])\n",
    "    return { \"text\" : texts }\n",
    "\n",
    "ft_dataset = ft_dataset.map(apply_chat_template, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b216ec-3a9c-4fd6-9e03-7d00cd082650",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_dataset[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a21cae-a558-4bf9-98e3-765a0da22646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = ft_dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 2, # Set this for 1 full training run.\n",
    "        max_steps = 30,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b04d1-f3d4-4017-ab16-0cb7698ab166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb4ec8-9494-462d-be23-fe6fa29ecb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08bac0d-c6f3-4b02-9f4c-0967794c279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d616a4b0-ce84-4f25-84a5-20c7842023bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e474fe-537c-4868-ab74-3a5e91575806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_code.data_utils.prompt_utils import prompt_generator\n",
    "dataset = GridDataset(grid_size=5, seed = 42, wall_symbol=\"#\", free_symbol=\".\")\n",
    "img, grid_world = dataset[1000]\n",
    "prompt_img = prompt_generator(grid_world, pure_language=False, img=None, img_symbol=\"This image\")\n",
    "print(prompt_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cae293-c0f5-4e23-bc53-85c7419bee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89daa93d-d423-454b-92bf-001efa2a7665",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(grid_world.a_star())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74bd71-8770-4d51-910c-2510cb00db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\n",
    "        \"type\" : \"text\",\n",
    "        \"text\" : prompt_img,\n",
    "    }]\n",
    "}]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ")\n",
    "outputs = model.generate(\n",
    "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    ")\n",
    "tokenizer.batch_decode(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
