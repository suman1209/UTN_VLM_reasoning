{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "114c902a-652d-4fcb-a8cd-a46224d2b714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from unsloth import FastModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "626d6624-b89c-404b-8d0f-7a92faea12f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.4: Fast Gemma3 patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4080. Num GPUs = 1. Max memory: 15.694 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "fourbit_models = [\n",
    "    # 4bit dynamic quants for superior accuracy and low memory use\n",
    "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
    "\n",
    "    # Other popular models!\n",
    "    \"unsloth/Llama-3.1-8B\",\n",
    "    \"unsloth/Llama-3.2-3B\",\n",
    "    \"unsloth/Llama-3.3-70B\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
    "    \"unsloth/Phi-4\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length = 2048, # Choose any for long context!\n",
    "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35d8aac0-49b9-48d1-a7b8-a0527f27d700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.language_model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # Turn off for just text!\n",
    "    finetune_language_layers   = True,  # Should leave on!\n",
    "    finetune_attention_modules = True,  # Attention good for GRPO\n",
    "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
    "\n",
    "    r = 8,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 8,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b9e164-a412-47f2-aa7b-4bc81ecb85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb38f262-c092-4685-a628-c5c4a4d12fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "## Env variables and preparation stuffs\n",
    "sys.path.insert(0, \"../\")\n",
    "from src_code.data_utils.dataset import GridDataset\n",
    "from src_code.data_utils.dataset_utils import dataset_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbf5f43f-7f48-44d9-ba2d-67034bbc210b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_list)=1000\n",
      "len(val_list)=2\n",
      "len(test_list)=2\n"
     ]
    }
   ],
   "source": [
    "sys_role = \"You are a helpful assistant working in path planning in a 2D grid.\"\n",
    "dataset = GridDataset(grid_size=5, seed = 42, wall_symbol=\"#\", free_symbol=\".\", cell_size=10)\n",
    "ft_dataset = dataset_generator(dataset, sys_role, train_num=1000, val_num=2, test_num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2103f38-3d9e-4f01-b71a-473e598ea8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_data_formats\n",
    "ft_dataset = standardize_data_formats(ft_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4baa5b5-66d4-4947-b40c-9b56fc075fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'You are a helpful assistant working in path planning in a 2D grid.',\n",
       "   'role': 'system'},\n",
       "  {'content': \"Here is the grid world:\\n # # # # # # #\\n # . . G . . #\\n # . . . . # #\\n # # # . . . #\\n # . . # S # #\\n # # . . . . #\\n # # # # # # #\\n\\nThe S cell is the starting cell,\\nthe G cell is the goal cell,\\nthe # cells are obstacles,\\nand the . cells are free cells.\\n\\nRules:\\nThe grid size is 5x5\\nThe path must not pass through the obstacles.\\nYou can move up, down, left, or right from one cell to another.\\nYou cannot move diagonally.\\nThe path must be the shortest path from the starting cell to the goal cell.\\nThe output should be a sequence of steps to reach the goal cell.\\nOutput the steps only.\\n\\nActions:\\nOnly give me the steps, like 'go up', 'go down', 'go left', 'go right' or 'not solvable'\\ngo up: move one cell up\\ngo down: move one cell down\\ngo left: move one cell left\\ngo right: move one cell right\\nnot solvable: it is not possible to go the the goal cell from the start cell\\n\\nOutput example:\\n('go up', 'go right', 'go right', 'go down', 'go right') or ('not solvable')\\n\\nCan you find the path from the starting cell to the goal cell?\\n\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"('go up', 'go up', 'go up', 'go left')\", 'role': 'assistant'}],\n",
       " 'text': \"<bos><start_of_turn>user\\nYou are a helpful assistant working in path planning in a 2D grid.\\n\\nHere is the grid world:\\n # # # # # # #\\n # . . G . . #\\n # . . . . # #\\n # # # . . . #\\n # . . # S # #\\n # # . . . . #\\n # # # # # # #\\n\\nThe S cell is the starting cell,\\nthe G cell is the goal cell,\\nthe # cells are obstacles,\\nand the . cells are free cells.\\n\\nRules:\\nThe grid size is 5x5\\nThe path must not pass through the obstacles.\\nYou can move up, down, left, or right from one cell to another.\\nYou cannot move diagonally.\\nThe path must be the shortest path from the starting cell to the goal cell.\\nThe output should be a sequence of steps to reach the goal cell.\\nOutput the steps only.\\n\\nActions:\\nOnly give me the steps, like 'go up', 'go down', 'go left', 'go right' or 'not solvable'\\ngo up: move one cell up\\ngo down: move one cell down\\ngo left: move one cell left\\ngo right: move one cell right\\nnot solvable: it is not possible to go the the goal cell from the start cell\\n\\nOutput example:\\n('go up', 'go right', 'go right', 'go down', 'go right') or ('not solvable')\\n\\nCan you find the path from the starting cell to the goal cell?<end_of_turn>\\n<start_of_turn>model\\n('go up', 'go up', 'go up', 'go left')<end_of_turn>\\n\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca4e5ff5-ccad-418d-a421-e9f977b7fe80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d86c4713c34f3987e7689783e0d4aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def apply_chat_template(examples):\n",
    "    texts = tokenizer.apply_chat_template(examples[\"messages\"])\n",
    "    return { \"text\" : texts }\n",
    "\n",
    "ft_dataset = ft_dataset.map(apply_chat_template, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7b216ec-3a9c-4fd6-9e03-7d00cd082650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<bos><start_of_turn>user\\nYou are a helpful assistant working in path planning in a 2D grid.\\n\\nHere is the grid world:\\n # # # # # # #\\n # . . G . . #\\n # . . . . # #\\n # # # . . . #\\n # . . # S # #\\n # # . . . . #\\n # # # # # # #\\n\\nThe S cell is the starting cell,\\nthe G cell is the goal cell,\\nthe # cells are obstacles,\\nand the . cells are free cells.\\n\\nRules:\\nThe grid size is 5x5\\nThe path must not pass through the obstacles.\\nYou can move up, down, left, or right from one cell to another.\\nYou cannot move diagonally.\\nThe path must be the shortest path from the starting cell to the goal cell.\\nThe output should be a sequence of steps to reach the goal cell.\\nOutput the steps only.\\n\\nActions:\\nOnly give me the steps, like 'go up', 'go down', 'go left', 'go right' or 'not solvable'\\ngo up: move one cell up\\ngo down: move one cell down\\ngo left: move one cell left\\ngo right: move one cell right\\nnot solvable: it is not possible to go the the goal cell from the start cell\\n\\nOutput example:\\n('go up', 'go right', 'go right', 'go down', 'go right') or ('not solvable')\\n\\nCan you find the path from the starting cell to the goal cell?<end_of_turn>\\n<start_of_turn>model\\n('go up', 'go up', 'go up', 'go left')<end_of_turn>\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_dataset[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10a21cae-a558-4bf9-98e3-765a0da22646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e024624ea63461282a7ee26796f3e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=24):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = ft_dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 100, # Set this for 1 full training run.\n",
    "        max_steps = 30,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "008b04d1-f3d4-4017-ab16-0cb7698ab166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399d07f2568945ab834af75d1aa39777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=24):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9fb4ec8-9494-462d-be23-fe6fa29ecb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<bos><bos><start_of_turn>user\\nYou are a helpful assistant working in path planning in a 2D grid.\\n\\nHere is the grid world:\\n # # # # # # #\\n # . . G . . #\\n # . . . . # #\\n # # # . . . #\\n # . . # S # #\\n # # . . . . #\\n # # # # # # #\\n\\nThe S cell is the starting cell,\\nthe G cell is the goal cell,\\nthe # cells are obstacles,\\nand the . cells are free cells.\\n\\nRules:\\nThe grid size is 5x5\\nThe path must not pass through the obstacles.\\nYou can move up, down, left, or right from one cell to another.\\nYou cannot move diagonally.\\nThe path must be the shortest path from the starting cell to the goal cell.\\nThe output should be a sequence of steps to reach the goal cell.\\nOutput the steps only.\\n\\nActions:\\nOnly give me the steps, like 'go up', 'go down', 'go left', 'go right' or 'not solvable'\\ngo up: move one cell up\\ngo down: move one cell down\\ngo left: move one cell left\\ngo right: move one cell right\\nnot solvable: it is not possible to go the the goal cell from the start cell\\n\\nOutput example:\\n('go up', 'go right', 'go right', 'go down', 'go right') or ('not solvable')\\n\\nCan you find the path from the starting cell to the goal cell?<end_of_turn>\\n<start_of_turn>model\\n('go up', 'go up', 'go up', 'go left')<end_of_turn>\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a08bac0d-c6f3-4b02-9f4c-0967794c279a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"                                                                                                                                                                                                                                                                                                                                              ('go up', 'go up', 'go up', 'go left')<end_of_turn>\\n\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d616a4b0-ce84-4f25-84a5-20c7842023bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 14,901,248/4,000,000,000 (0.37% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:33, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.313600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.308300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.423500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.314300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.365300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.420400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.414900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.390500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.387900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.353100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.373600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.415600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.405700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.380700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.370600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.388200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.402900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.331300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.358300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.340700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.332600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "00e474fe-537c-4868-ab74-3a5e91575806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the grid world:\n",
      " # # # # # # #\n",
      " # . . G . . #\n",
      " # . . . . # #\n",
      " # # # . . . #\n",
      " # . . # S # #\n",
      " # # . . . . #\n",
      " # # # # # # #\n",
      "\n",
      "The S cell is the starting cell,\n",
      "the G cell is the goal cell,\n",
      "the # cells are obstacles,\n",
      "and the . cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The grid size is 5x5\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "Output the steps only.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left', 'go right' or 'not solvable'\n",
      "go up: move one cell up\n",
      "go down: move one cell down\n",
      "go left: move one cell left\n",
      "go right: move one cell right\n",
      "not solvable: it is not possible to go the the goal cell from the start cell\n",
      "\n",
      "Output example:\n",
      "('go up', 'go right', 'go right', 'go down', 'go right') or ('not solvable')\n",
      "\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src_code.data_utils.prompt_utils import prompt_generator\n",
    "dataset = GridDataset(grid_size=5, seed = 42, wall_symbol=\"#\", free_symbol=\".\")\n",
    "img, grid_world = dataset[0]\n",
    "prompt_img = prompt_generator(grid_world, pure_language=False, img=None, img_symbol=\"This image\")\n",
    "print(prompt_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5cae293-c0f5-4e23-bc53-85c7419bee23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABGAEYDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD0PwX4L8K3XgXw9cXHhrRpp5dMtnkkksImZ2MSkkkrkknnNYfxJ8J+G7H/AIRH7H4f0q38/wAS2cE3k2UaeZG2/cjYHKnAyDwaPCfxJ+w+DdDs/wDhCvGVx5Gn28XnW+lb45NsajcjbuVOMg9xWP4/8f8A9qf8Iv8A8Ul4rs/sviC0uv8AS9N8vztu793H8x3SHPC98GgD1D/hBPB//QqaH/4Lof8A4mvJfGfhzQ7X4uw2VvounQ2h0FZTBHaoqF/PYbtoGM44z1xXdf8AC0/+pC8c/wDgn/8As6838S+KZ9Z+Kcd/b+FPEyuuiiE2cmnEXGPPJ8zYCfk5xu9eK5MdGcsPONPexxZlCpPCTjS+JrSxc/4RzQ/+gNp3/gKn+FbPwh8LeHtT0fxC9/oOl3bxa9cxRtPZxyFECx4UZHAGTx05rB/tnVP+hK8W/wDgqb/GrPwx8btoGna9bv4S8U3ry61cTMbLTvMERIQeW/zDa4xyvbIrysnoYmnOTrJpW6niZBhsXSqTeITStpc7zxp4L8K2vgXxDcW/hrRoZ4tMuXjkjsIlZGETEEELkEHnNHgvwX4VuvAvh64uPDWjTTy6ZbPJJJYRMzsYlJJJXJJPOaw/FnxJ+3eDdcs/+EK8ZW/n6fcRedcaVsjj3RsNztu4UZyT2FUfC3xj0rS/COi6fJ4d8Syva2EEDSQ2SsjlY1UlTvGQccGvoD6go/H3w1oOjeBbG40vRNNsZ21ONGktbVImK+VKcEqAcZAOPYUVhfGL4jWHi/wjaafa6Nrdk8d+k5kv7VY0IEci4BDHn5unsaKAPcfAn/JPPDX/AGCrX/0Utc/8U/8AmSv+xrsf/Z68+8PePvH9n4Z0q1s7fw0bWGzhjhMyT7ygQBd2GxnAGcVm+L/GvjbUf7B/tKDw+v2bWLe4tvsyTDMy7tofc33OTnHNAH0nXn//ADcL/wByp/7d1yH/AAsf4j/8+3hT/vi4/wDiqzdE1vx/r3xYe6tU8NJrCaGYyJROLfyBOD2JbfuP0x70AfQNef8Aws/5nX/sa77/ANko/wCLv/8AUjf+Tdcf4A/4WP8A8VR/ZH/CKf8AIwXf2z7X9o/4+Pl3+Xt/5Z9MZ565oA9Q8d/8k88S/wDYKuv/AEU1eIaD/wAi9pn/AF6Rf+gCu38Wf8LT/wCEN1z+0f8AhDfsP9n3H2j7P9q8zy/LbdszxuxnGeM1y3h7wD4/vPDOlXVnceGhazWcMkImefeEKAruwuM4IzigDkfiP/yL1v8A9fa/+gPRVr4oeE/Fug+Gba616bRHtXvFjQWDSl95RyM7wBjAP6UUAa2g/wDIvaZ/16Rf+gCs/wAVf8wT/sKwf+zV3Phb4OaVqnhHRdQk8ReJYnurCCdo4b1VRC0asQo2HAGeBWV45+FWm6H/AMI35Ou+IJ/t2u2tk/2m7V/LV92XTCDDjHB5+lAEdaHw4/5LJcf9i+3/AKUJW/8A8KO0f/oZ/Ff/AIHp/wDG6888R+EP+EV+Jcem6X4i8QR+Zo4na5+24n5mK7NyqPk+UHGOvNAH0nXn/wALP+Z1/wCxrvv/AGSvMPsesf8AQ6eK/wDwaPXH6bc6xbXOqx2viXXLYDUJt/kXzp5r5GXfH3nPc98VnUqRpx5pHXgsFVxtX2NHc+n/AB3/AMk88S/9gq6/9FNR4E/5J54a/wCwVa/+ilr5l1K+119KvFm8VeIJojA4eKXUHZHG05VgeoPQit3RbTVW0LT2j8W+JYENtGVih1JlSMbRhVHYDoBSpVo1VeJpj8tr4CSjXtd9nc739o7/AJJ5p/8A2FY//RUtFeTePre/i0KBrrxBreop9pUCK/vWmQHa3zAHv2z7mitTgPffBfjTwra+BfD1vceJdGhni0y2SSOS/iVkYRKCCC2QQeMVh/EnxZ4bvv8AhEfsfiDSrjyPEtnPN5N7G/lxrv3O2DwoyMk8CvOtF0XSpdC0+STTLN3e2jZmaBSSSoyScUXei6Uuu+F410yzCTa5aRSqIFxIhblWGOQe4NAHvv8Awnfg/wD6GvQ//BjD/wDFV494+8Q6JefFiC8tdY0+e1GhrEZ4rlGQP57HbuBxnBzivYf+EE8H/wDQqaH/AOC6H/4muH/4RPw3/wAL1/s7/hH9K+w/8I15/wBm+xR+X5n2nbv24xuxxnrigDiP7e0f/oLWP/gQn+NcPaX9mt5qrNdwAPfyuhMg+ZSeCPUV9T/8IJ4P/wChU0P/AMF0P/xNcP8ADbwn4bvv+Eu+2eH9KuPI8S3kEPnWUb+XGuzai5HCjJwBwKyrUlVjys78tx8sBX9vFXdmvvPEr+/s3066VbuBmaFwAJASTg+9dRoutaVFoWnxyanZo6W0asrTqCCFGQRmvYfGngvwra+BfENxb+GtGhni0y5eOSOwiVkYRMQQQuQQec0eC/BfhW68C+Hri48NaNNPLpls8kklhEzOxiUkklckk85qaFBUU0nc3zbNp5lOM5xSsraHgXj/AFOwvNCgjtb22ncXKsVilViBtbnAPvRXovx98NaDo3gWxuNL0TTbGdtTjRpLW1SJivlSnBKgHGQDj2FFbnknnWm+P9Ks9Ks7WS3vC8MCRsVRcEhQDj5vai48faVLquh3S294EsNUt7yUFFyURskL83X0zj60UUAes/8ADR3g/wD6Buuf9+If/jtcx/wuvw3/AMLT/wCEo+xar9h/sT+z/L8qPzPM8/zM48zG3HfOc9qKKAOn/wCGjvB//QN1z/vxD/8AHa5jwX8a/Dfhz/hIftllqr/2lrdzqEPkxRnbHJt2hsyDDcHIGR70UUAafiX4++FdZ8K6vpdvp+srPe2U1vG0kMQUM6FQTiQnGT6Gjw18ffCujeFdI0u40/WWnsrKG3kaOGIqWRApIzIDjI9BRRQBzHxZ+LOg+PPCtrpel2mpQzxXqXDNdRoqlQjrgbXY5y47etFFFAH/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEYAAABGCAIAAAD+THXTAAABC0lEQVR4Ae2aQQ6DMAwEQ1/Oz9Pwgl1LQWui6XkV2zOADOoYx/2uNdGcU851XbXk0EeO8Ry5v/pPDvO5ACN9QRmWsBQhwIUXwV4siqUisEj8QEsRjhQtEqjt12/s7P6Z933L6VbmwHuJkaT4BgEsNZAgW8CSRNQggKUGEmQLWJKICEDAIvBs4uaGW0r6+/X2JI8Hy3w4hKWwAKs8lixM4RCWwgKs8liyMIVDB1oKE6W8ReCtb+L+dm//jcV9YzjwXmIk62IOh7AUFmCVx5KFKRzCUliAVR5LFiZCEJAEat/Et3+/Xv1tP5PHg7TeIIClBhJkC1iSiBoEsNRAgmwBSxJRg8CBlhpQ3d3CH2X3R3SrNfSGAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=70x70>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89daa93d-d423-454b-92bf-001efa2a7665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"('go up', 'go up', 'go up', 'go left')\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(grid_world.a_star())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd2d6750-28a5-42fd-acd1-700583226bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma3ForConditionalGeneration(\n",
       "      (vision_tower): SiglipVisionModel(\n",
       "        (vision_model): SiglipVisionTransformer(\n",
       "          (embeddings): SiglipVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "            (position_embedding): Embedding(4096, 1152)\n",
       "          )\n",
       "          (encoder): SiglipEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-26): 27 x SiglipEncoderLayer(\n",
       "                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                (self_attn): SiglipAttention(\n",
       "                  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): SiglipMLP(\n",
       "                  (activation_fn): PytorchGELUTanh()\n",
       "                  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (multi_modal_projector): Gemma3MultiModalProjector(\n",
       "        (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "      )\n",
       "      (language_model): Gemma3ForCausalLM(\n",
       "        (model): Gemma3TextModel(\n",
       "          (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "            (2): Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "            (3-5): 3 x Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "            (6-33): 28 x Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Identity()\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "          )\n",
       "          (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (rotary_emb): Gemma3RotaryEmbedding()\n",
       "          (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ce74bd71-8770-4d51-910c-2510cb00db16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<bos><start_of_turn>user\\nHere is the grid world:\\n # # # # # # #\\n # . . G . . #\\n # . . . . # #\\n # # # . . . #\\n # . . # S # #\\n # # . . . . #\\n # # # # # # #\\n\\nThe S cell is the starting cell,\\nthe G cell is the goal cell,\\nthe # cells are obstacles,\\nand the . cells are free cells.\\n\\nRules:\\nThe grid size is 5x5\\nThe path must not pass through the obstacles.\\nYou can move up, down, left, or right from one cell to another.\\nYou cannot move diagonally.\\nThe path must be the shortest path from the starting cell to the goal cell.\\nThe output should be a sequence of steps to reach the goal cell.\\nOutput the steps only.\\n\\nActions:\\nOnly give me the steps, like 'go up', 'go down', 'go left', 'go right' or 'not solvable'\\ngo up: move one cell up\\ngo down: move one cell down\\ngo left: move one cell left\\ngo right: move one cell right\\nnot solvable: it is not possible to go the the goal cell from the start cell\\n\\nOutput example:\\n('go up', 'go right', 'go right', 'go down', 'go right') or ('not solvable')\\n\\nCan you find the path from the starting cell to the goal cell?<end_of_turn>\\n<start_of_turn>model\\n('go left', 'go left', 'go down')<end_of_turn>\"]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"gemma-3\",\n",
    ")\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\n",
    "        \"type\" : \"text\",\n",
    "        \"text\" : prompt_img,\n",
    "    }]\n",
    "}]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ")\n",
    "outputs = model.generate(\n",
    "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 64, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    ")\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ff26fd-87e7-4283-9b69-d8aac0e290b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
