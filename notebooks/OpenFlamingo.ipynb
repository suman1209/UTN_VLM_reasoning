{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0833d1e6-e58c-4d66-bce9-1ad02b7ef476",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "## other standard packages\n",
    "import sys\n",
    "## Env variables and preparation stuffs\n",
    "sys.path.insert(0, \"../\")\n",
    "from src_code.data_utils.dataset import GridDataset\n",
    "from src_code.data_utils.dataset_utils import CellType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8137977c-ca7b-46a2-8e14-6dcab4274c35",
   "metadata": {},
   "source": [
    "# 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aa20a2b-d8b0-40d1-a90a-bf42b52e1f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GridDataset(grid_size=5, seed = 42, wall_symbol=\"#\", free_symbol=\".\")\n",
    "\n",
    "img_rgb1, grid_world1 = dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0b796b9-dce1-47a0-aa81-ad02f6633911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEYAAABGCAIAAAD+THXTAAABAklEQVR4Ae2aUQqDQBDF1p7cm1ulB9gIluCSfj/WN4nKUBxjud92TrTv+3SuX+Y4jmly264zxzw4xi+Iz4RX/0wrvi7QSG9QlqUsKQS68RTsNy+apZvAlPiClhSOXfQmgWsZhhuunwSznfMs+Cw1ElCvR7KkKwAFsgQg6ZEs6QpAgSwBSEUiAAi8ahNn/573egDe9UiWdAWgQJYAJD2SJV0BKJAlAEmPLGhJZ1oBQODeJv6P71geP3PBZ6mRwK2sR7KkKwAFsgQg6ZEs6QpAgSwBSEUiAAhcm/jju/B5pvjFS68H4F2PZElXAApkCUDSI1nSFYACWQKQ9MiClnSmzxf4AvptR3RGg3+4AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=70x70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_rgb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c48695ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# . G . . \n",
      ". . . . S \n",
      ". . . . . \n",
      ". # . # . \n",
      "# . . . . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(str(grid_world1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baf9f9f1-1e1e-43ba-b9e9-f1995971804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid_world1.a_star() = ('go up', 'go left', 'go left')\n"
     ]
    }
   ],
   "source": [
    "print(f\"{grid_world1.a_star() = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac2375-697b-44ea-8600-f22a8e981947",
   "metadata": {},
   "source": [
    "## 3 Open Flamingo Inference\n",
    "Source: https://github.com/mlfoundations/open_flamingo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28380ad-1873-4b86-8b53-bd02c3558769",
   "metadata": {},
   "source": [
    "## 3.1  Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f9d9b7d-3cbf-4cd3-8019-03f1039ea778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/lit2425/jenga/suman/UTN_VLM_reasoning/vlm/lib/python3.10/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n",
      "/var/lit2425/jenga/suman/UTN_VLM_reasoning/vlm/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n",
      "/var/lit2425/jenga/suman/UTN_VLM_reasoning/vlm/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 50280. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flamingo model initialized with 1089480736 trainable parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['vision_encoder.class_embedding', 'vision_encoder.positional_embedding', 'vision_encoder.proj', 'vision_encoder.conv1.weight', 'vision_encoder.ln_pre.weight', 'vision_encoder.ln_pre.bias', 'vision_encoder.transformer.resblocks.0.ln_1.weight', 'vision_encoder.transformer.resblocks.0.ln_1.bias', 'vision_encoder.transformer.resblocks.0.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.0.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.0.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.0.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.0.ln_2.weight', 'vision_encoder.transformer.resblocks.0.ln_2.bias', 'vision_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.1.ln_1.weight', 'vision_encoder.transformer.resblocks.1.ln_1.bias', 'vision_encoder.transformer.resblocks.1.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.1.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.1.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.1.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.1.ln_2.weight', 'vision_encoder.transformer.resblocks.1.ln_2.bias', 'vision_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.2.ln_1.weight', 'vision_encoder.transformer.resblocks.2.ln_1.bias', 'vision_encoder.transformer.resblocks.2.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.2.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.2.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.2.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.2.ln_2.weight', 'vision_encoder.transformer.resblocks.2.ln_2.bias', 'vision_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.3.ln_1.weight', 'vision_encoder.transformer.resblocks.3.ln_1.bias', 'vision_encoder.transformer.resblocks.3.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.3.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.3.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.3.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.3.ln_2.weight', 'vision_encoder.transformer.resblocks.3.ln_2.bias', 'vision_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.4.ln_1.weight', 'vision_encoder.transformer.resblocks.4.ln_1.bias', 'vision_encoder.transformer.resblocks.4.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.4.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.4.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.4.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.4.ln_2.weight', 'vision_encoder.transformer.resblocks.4.ln_2.bias', 'vision_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.5.ln_1.weight', 'vision_encoder.transformer.resblocks.5.ln_1.bias', 'vision_encoder.transformer.resblocks.5.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.5.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.5.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.5.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.5.ln_2.weight', 'vision_encoder.transformer.resblocks.5.ln_2.bias', 'vision_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.6.ln_1.weight', 'vision_encoder.transformer.resblocks.6.ln_1.bias', 'vision_encoder.transformer.resblocks.6.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.6.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.6.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.6.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.6.ln_2.weight', 'vision_encoder.transformer.resblocks.6.ln_2.bias', 'vision_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.7.ln_1.weight', 'vision_encoder.transformer.resblocks.7.ln_1.bias', 'vision_encoder.transformer.resblocks.7.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.7.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.7.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.7.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.7.ln_2.weight', 'vision_encoder.transformer.resblocks.7.ln_2.bias', 'vision_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.8.ln_1.weight', 'vision_encoder.transformer.resblocks.8.ln_1.bias', 'vision_encoder.transformer.resblocks.8.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.8.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.8.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.8.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.8.ln_2.weight', 'vision_encoder.transformer.resblocks.8.ln_2.bias', 'vision_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.9.ln_1.weight', 'vision_encoder.transformer.resblocks.9.ln_1.bias', 'vision_encoder.transformer.resblocks.9.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.9.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.9.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.9.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.9.ln_2.weight', 'vision_encoder.transformer.resblocks.9.ln_2.bias', 'vision_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.10.ln_1.weight', 'vision_encoder.transformer.resblocks.10.ln_1.bias', 'vision_encoder.transformer.resblocks.10.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.10.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.10.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.10.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.10.ln_2.weight', 'vision_encoder.transformer.resblocks.10.ln_2.bias', 'vision_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.11.ln_1.weight', 'vision_encoder.transformer.resblocks.11.ln_1.bias', 'vision_encoder.transformer.resblocks.11.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.11.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.11.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.11.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.11.ln_2.weight', 'vision_encoder.transformer.resblocks.11.ln_2.bias', 'vision_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.12.ln_1.weight', 'vision_encoder.transformer.resblocks.12.ln_1.bias', 'vision_encoder.transformer.resblocks.12.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.12.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.12.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.12.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.12.ln_2.weight', 'vision_encoder.transformer.resblocks.12.ln_2.bias', 'vision_encoder.transformer.resblocks.12.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.12.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.12.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.12.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.13.ln_1.weight', 'vision_encoder.transformer.resblocks.13.ln_1.bias', 'vision_encoder.transformer.resblocks.13.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.13.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.13.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.13.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.13.ln_2.weight', 'vision_encoder.transformer.resblocks.13.ln_2.bias', 'vision_encoder.transformer.resblocks.13.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.13.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.13.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.13.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.14.ln_1.weight', 'vision_encoder.transformer.resblocks.14.ln_1.bias', 'vision_encoder.transformer.resblocks.14.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.14.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.14.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.14.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.14.ln_2.weight', 'vision_encoder.transformer.resblocks.14.ln_2.bias', 'vision_encoder.transformer.resblocks.14.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.14.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.14.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.14.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.15.ln_1.weight', 'vision_encoder.transformer.resblocks.15.ln_1.bias', 'vision_encoder.transformer.resblocks.15.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.15.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.15.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.15.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.15.ln_2.weight', 'vision_encoder.transformer.resblocks.15.ln_2.bias', 'vision_encoder.transformer.resblocks.15.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.15.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.15.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.15.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.16.ln_1.weight', 'vision_encoder.transformer.resblocks.16.ln_1.bias', 'vision_encoder.transformer.resblocks.16.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.16.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.16.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.16.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.16.ln_2.weight', 'vision_encoder.transformer.resblocks.16.ln_2.bias', 'vision_encoder.transformer.resblocks.16.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.16.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.16.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.16.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.17.ln_1.weight', 'vision_encoder.transformer.resblocks.17.ln_1.bias', 'vision_encoder.transformer.resblocks.17.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.17.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.17.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.17.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.17.ln_2.weight', 'vision_encoder.transformer.resblocks.17.ln_2.bias', 'vision_encoder.transformer.resblocks.17.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.17.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.17.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.17.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.18.ln_1.weight', 'vision_encoder.transformer.resblocks.18.ln_1.bias', 'vision_encoder.transformer.resblocks.18.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.18.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.18.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.18.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.18.ln_2.weight', 'vision_encoder.transformer.resblocks.18.ln_2.bias', 'vision_encoder.transformer.resblocks.18.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.18.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.18.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.18.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.19.ln_1.weight', 'vision_encoder.transformer.resblocks.19.ln_1.bias', 'vision_encoder.transformer.resblocks.19.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.19.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.19.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.19.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.19.ln_2.weight', 'vision_encoder.transformer.resblocks.19.ln_2.bias', 'vision_encoder.transformer.resblocks.19.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.19.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.19.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.19.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.20.ln_1.weight', 'vision_encoder.transformer.resblocks.20.ln_1.bias', 'vision_encoder.transformer.resblocks.20.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.20.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.20.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.20.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.20.ln_2.weight', 'vision_encoder.transformer.resblocks.20.ln_2.bias', 'vision_encoder.transformer.resblocks.20.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.20.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.20.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.20.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.21.ln_1.weight', 'vision_encoder.transformer.resblocks.21.ln_1.bias', 'vision_encoder.transformer.resblocks.21.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.21.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.21.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.21.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.21.ln_2.weight', 'vision_encoder.transformer.resblocks.21.ln_2.bias', 'vision_encoder.transformer.resblocks.21.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.21.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.21.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.21.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.22.ln_1.weight', 'vision_encoder.transformer.resblocks.22.ln_1.bias', 'vision_encoder.transformer.resblocks.22.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.22.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.22.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.22.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.22.ln_2.weight', 'vision_encoder.transformer.resblocks.22.ln_2.bias', 'vision_encoder.transformer.resblocks.22.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.22.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.22.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.22.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.23.ln_1.weight', 'vision_encoder.transformer.resblocks.23.ln_1.bias', 'vision_encoder.transformer.resblocks.23.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.23.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.23.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.23.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.23.ln_2.weight', 'vision_encoder.transformer.resblocks.23.ln_2.bias', 'vision_encoder.transformer.resblocks.23.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.23.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.23.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.23.mlp.c_proj.bias', 'vision_encoder.ln_post.weight', 'vision_encoder.ln_post.bias', 'lang_encoder.gpt_neox.layers.0.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.0.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.0.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.0.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.0.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.0.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.0.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.0.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.0.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.0.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.0.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.0.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.0.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.1.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.1.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.1.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.1.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.1.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.1.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.1.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.1.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.1.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.1.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.1.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.1.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.1.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.2.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.2.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.2.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.2.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.2.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.2.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.2.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.2.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.2.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.2.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.2.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.2.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.2.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.3.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.3.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.3.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.3.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.3.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.3.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.3.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.3.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.3.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.3.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.3.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.3.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.3.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.4.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.4.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.4.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.4.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.4.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.4.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.4.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.4.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.4.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.4.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.4.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.4.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.4.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.5.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.5.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.5.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.5.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.5.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.5.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.5.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.5.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.5.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.5.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.5.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.5.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.5.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.6.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.6.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.6.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.6.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.6.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.6.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.6.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.6.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.6.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.6.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.6.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.6.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.6.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.7.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.7.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.7.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.7.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.7.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.7.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.7.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.7.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.7.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.7.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.7.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.7.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.7.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.8.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.8.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.8.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.8.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.8.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.8.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.8.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.8.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.8.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.8.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.8.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.8.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.8.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.9.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.9.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.9.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.9.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.9.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.9.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.9.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.9.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.9.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.9.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.9.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.9.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.9.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.10.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.10.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.10.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.10.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.10.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.10.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.10.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.10.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.10.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.10.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.10.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.10.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.10.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.11.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.11.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.11.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.11.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.11.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.11.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.11.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.11.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.11.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.11.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.11.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.11.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.11.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.12.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.12.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.12.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.12.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.12.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.12.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.12.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.12.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.12.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.12.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.12.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.12.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.12.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.13.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.13.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.13.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.13.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.13.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.13.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.13.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.13.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.13.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.13.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.13.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.13.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.13.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.14.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.14.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.14.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.14.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.14.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.14.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.14.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.14.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.14.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.14.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.14.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.14.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.14.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.15.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.15.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.15.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.15.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.15.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.15.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.15.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.15.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.15.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.15.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.15.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.15.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.15.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.16.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.16.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.16.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.16.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.16.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.16.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.16.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.16.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.16.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.16.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.16.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.16.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.16.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.17.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.17.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.17.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.17.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.17.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.17.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.17.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.17.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.17.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.17.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.17.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.17.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.17.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.18.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.18.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.18.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.18.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.18.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.18.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.18.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.18.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.18.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.18.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.18.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.18.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.18.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.19.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.19.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.19.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.19.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.19.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.19.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.19.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.19.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.19.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.19.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.19.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.19.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.19.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.20.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.20.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.20.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.20.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.20.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.20.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.20.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.20.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.20.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.20.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.20.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.20.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.20.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.21.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.21.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.21.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.21.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.21.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.21.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.21.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.21.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.21.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.21.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.21.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.21.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.21.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.22.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.22.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.22.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.22.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.22.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.22.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.22.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.22.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.22.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.22.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.22.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.22.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.22.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.23.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.23.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.23.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.23.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.23.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.23.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.23.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.23.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.23.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.23.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.23.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.23.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.23.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.24.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.24.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.24.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.24.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.24.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.24.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.24.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.24.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.24.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.24.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.24.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.24.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.24.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.25.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.25.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.25.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.25.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.25.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.25.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.25.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.25.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.25.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.25.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.25.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.25.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.25.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.26.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.26.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.26.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.26.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.26.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.26.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.26.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.26.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.26.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.26.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.26.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.26.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.26.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.27.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.27.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.27.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.27.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.27.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.27.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.27.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.27.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.27.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.27.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.27.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.27.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.27.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.28.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.28.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.28.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.28.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.28.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.28.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.28.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.28.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.28.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.28.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.28.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.28.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.28.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.29.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.29.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.29.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.29.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.29.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.29.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.29.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.29.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.29.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.29.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.29.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.29.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.29.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.30.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.30.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.30.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.30.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.30.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.30.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.30.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.30.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.30.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.30.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.30.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.30.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.30.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.layers.31.decoder_layer.input_layernorm.weight', 'lang_encoder.gpt_neox.layers.31.decoder_layer.input_layernorm.bias', 'lang_encoder.gpt_neox.layers.31.decoder_layer.post_attention_layernorm.weight', 'lang_encoder.gpt_neox.layers.31.decoder_layer.post_attention_layernorm.bias', 'lang_encoder.gpt_neox.layers.31.decoder_layer.attention.rotary_emb.inv_freq', 'lang_encoder.gpt_neox.layers.31.decoder_layer.attention.query_key_value.weight', 'lang_encoder.gpt_neox.layers.31.decoder_layer.attention.query_key_value.bias', 'lang_encoder.gpt_neox.layers.31.decoder_layer.attention.dense.weight', 'lang_encoder.gpt_neox.layers.31.decoder_layer.attention.dense.bias', 'lang_encoder.gpt_neox.layers.31.decoder_layer.mlp.dense_h_to_4h.weight', 'lang_encoder.gpt_neox.layers.31.decoder_layer.mlp.dense_h_to_4h.bias', 'lang_encoder.gpt_neox.layers.31.decoder_layer.mlp.dense_4h_to_h.weight', 'lang_encoder.gpt_neox.layers.31.decoder_layer.mlp.dense_4h_to_h.bias', 'lang_encoder.gpt_neox.final_layer_norm.weight', 'lang_encoder.gpt_neox.final_layer_norm.bias', 'lang_encoder.old_decoder_blocks.0.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.0.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.0.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.0.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.0.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.0.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.0.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.0.attention.dense.weight', 'lang_encoder.old_decoder_blocks.0.attention.dense.bias', 'lang_encoder.old_decoder_blocks.0.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.0.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.0.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.0.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.1.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.1.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.1.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.1.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.1.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.1.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.1.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.1.attention.dense.weight', 'lang_encoder.old_decoder_blocks.1.attention.dense.bias', 'lang_encoder.old_decoder_blocks.1.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.1.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.1.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.1.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.2.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.2.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.2.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.2.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.2.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.2.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.2.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.2.attention.dense.weight', 'lang_encoder.old_decoder_blocks.2.attention.dense.bias', 'lang_encoder.old_decoder_blocks.2.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.2.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.2.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.2.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.3.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.3.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.3.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.3.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.3.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.3.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.3.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.3.attention.dense.weight', 'lang_encoder.old_decoder_blocks.3.attention.dense.bias', 'lang_encoder.old_decoder_blocks.3.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.3.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.3.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.3.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.4.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.4.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.4.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.4.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.4.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.4.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.4.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.4.attention.dense.weight', 'lang_encoder.old_decoder_blocks.4.attention.dense.bias', 'lang_encoder.old_decoder_blocks.4.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.4.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.4.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.4.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.5.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.5.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.5.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.5.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.5.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.5.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.5.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.5.attention.dense.weight', 'lang_encoder.old_decoder_blocks.5.attention.dense.bias', 'lang_encoder.old_decoder_blocks.5.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.5.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.5.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.5.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.6.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.6.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.6.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.6.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.6.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.6.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.6.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.6.attention.dense.weight', 'lang_encoder.old_decoder_blocks.6.attention.dense.bias', 'lang_encoder.old_decoder_blocks.6.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.6.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.6.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.6.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.7.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.7.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.7.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.7.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.7.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.7.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.7.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.7.attention.dense.weight', 'lang_encoder.old_decoder_blocks.7.attention.dense.bias', 'lang_encoder.old_decoder_blocks.7.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.7.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.7.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.7.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.8.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.8.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.8.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.8.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.8.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.8.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.8.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.8.attention.dense.weight', 'lang_encoder.old_decoder_blocks.8.attention.dense.bias', 'lang_encoder.old_decoder_blocks.8.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.8.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.8.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.8.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.9.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.9.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.9.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.9.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.9.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.9.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.9.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.9.attention.dense.weight', 'lang_encoder.old_decoder_blocks.9.attention.dense.bias', 'lang_encoder.old_decoder_blocks.9.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.9.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.9.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.9.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.10.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.10.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.10.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.10.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.10.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.10.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.10.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.10.attention.dense.weight', 'lang_encoder.old_decoder_blocks.10.attention.dense.bias', 'lang_encoder.old_decoder_blocks.10.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.10.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.10.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.10.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.11.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.11.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.11.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.11.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.11.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.11.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.11.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.11.attention.dense.weight', 'lang_encoder.old_decoder_blocks.11.attention.dense.bias', 'lang_encoder.old_decoder_blocks.11.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.11.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.11.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.11.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.12.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.12.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.12.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.12.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.12.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.12.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.12.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.12.attention.dense.weight', 'lang_encoder.old_decoder_blocks.12.attention.dense.bias', 'lang_encoder.old_decoder_blocks.12.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.12.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.12.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.12.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.13.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.13.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.13.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.13.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.13.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.13.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.13.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.13.attention.dense.weight', 'lang_encoder.old_decoder_blocks.13.attention.dense.bias', 'lang_encoder.old_decoder_blocks.13.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.13.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.13.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.13.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.14.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.14.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.14.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.14.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.14.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.14.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.14.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.14.attention.dense.weight', 'lang_encoder.old_decoder_blocks.14.attention.dense.bias', 'lang_encoder.old_decoder_blocks.14.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.14.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.14.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.14.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.15.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.15.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.15.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.15.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.15.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.15.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.15.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.15.attention.dense.weight', 'lang_encoder.old_decoder_blocks.15.attention.dense.bias', 'lang_encoder.old_decoder_blocks.15.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.15.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.15.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.15.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.16.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.16.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.16.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.16.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.16.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.16.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.16.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.16.attention.dense.weight', 'lang_encoder.old_decoder_blocks.16.attention.dense.bias', 'lang_encoder.old_decoder_blocks.16.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.16.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.16.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.16.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.17.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.17.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.17.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.17.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.17.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.17.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.17.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.17.attention.dense.weight', 'lang_encoder.old_decoder_blocks.17.attention.dense.bias', 'lang_encoder.old_decoder_blocks.17.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.17.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.17.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.17.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.18.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.18.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.18.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.18.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.18.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.18.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.18.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.18.attention.dense.weight', 'lang_encoder.old_decoder_blocks.18.attention.dense.bias', 'lang_encoder.old_decoder_blocks.18.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.18.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.18.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.18.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.19.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.19.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.19.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.19.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.19.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.19.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.19.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.19.attention.dense.weight', 'lang_encoder.old_decoder_blocks.19.attention.dense.bias', 'lang_encoder.old_decoder_blocks.19.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.19.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.19.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.19.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.20.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.20.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.20.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.20.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.20.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.20.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.20.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.20.attention.dense.weight', 'lang_encoder.old_decoder_blocks.20.attention.dense.bias', 'lang_encoder.old_decoder_blocks.20.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.20.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.20.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.20.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.21.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.21.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.21.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.21.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.21.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.21.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.21.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.21.attention.dense.weight', 'lang_encoder.old_decoder_blocks.21.attention.dense.bias', 'lang_encoder.old_decoder_blocks.21.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.21.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.21.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.21.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.22.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.22.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.22.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.22.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.22.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.22.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.22.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.22.attention.dense.weight', 'lang_encoder.old_decoder_blocks.22.attention.dense.bias', 'lang_encoder.old_decoder_blocks.22.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.22.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.22.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.22.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.23.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.23.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.23.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.23.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.23.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.23.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.23.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.23.attention.dense.weight', 'lang_encoder.old_decoder_blocks.23.attention.dense.bias', 'lang_encoder.old_decoder_blocks.23.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.23.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.23.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.23.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.24.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.24.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.24.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.24.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.24.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.24.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.24.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.24.attention.dense.weight', 'lang_encoder.old_decoder_blocks.24.attention.dense.bias', 'lang_encoder.old_decoder_blocks.24.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.24.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.24.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.24.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.25.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.25.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.25.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.25.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.25.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.25.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.25.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.25.attention.dense.weight', 'lang_encoder.old_decoder_blocks.25.attention.dense.bias', 'lang_encoder.old_decoder_blocks.25.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.25.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.25.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.25.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.26.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.26.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.26.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.26.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.26.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.26.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.26.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.26.attention.dense.weight', 'lang_encoder.old_decoder_blocks.26.attention.dense.bias', 'lang_encoder.old_decoder_blocks.26.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.26.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.26.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.26.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.27.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.27.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.27.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.27.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.27.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.27.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.27.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.27.attention.dense.weight', 'lang_encoder.old_decoder_blocks.27.attention.dense.bias', 'lang_encoder.old_decoder_blocks.27.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.27.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.27.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.27.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.28.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.28.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.28.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.28.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.28.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.28.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.28.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.28.attention.dense.weight', 'lang_encoder.old_decoder_blocks.28.attention.dense.bias', 'lang_encoder.old_decoder_blocks.28.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.28.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.28.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.28.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.29.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.29.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.29.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.29.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.29.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.29.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.29.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.29.attention.dense.weight', 'lang_encoder.old_decoder_blocks.29.attention.dense.bias', 'lang_encoder.old_decoder_blocks.29.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.29.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.29.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.29.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.30.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.30.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.30.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.30.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.30.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.30.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.30.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.30.attention.dense.weight', 'lang_encoder.old_decoder_blocks.30.attention.dense.bias', 'lang_encoder.old_decoder_blocks.30.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.30.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.30.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.30.mlp.dense_4h_to_h.bias', 'lang_encoder.old_decoder_blocks.31.input_layernorm.weight', 'lang_encoder.old_decoder_blocks.31.input_layernorm.bias', 'lang_encoder.old_decoder_blocks.31.post_attention_layernorm.weight', 'lang_encoder.old_decoder_blocks.31.post_attention_layernorm.bias', 'lang_encoder.old_decoder_blocks.31.attention.rotary_emb.inv_freq', 'lang_encoder.old_decoder_blocks.31.attention.query_key_value.weight', 'lang_encoder.old_decoder_blocks.31.attention.query_key_value.bias', 'lang_encoder.old_decoder_blocks.31.attention.dense.weight', 'lang_encoder.old_decoder_blocks.31.attention.dense.bias', 'lang_encoder.old_decoder_blocks.31.mlp.dense_h_to_4h.weight', 'lang_encoder.old_decoder_blocks.31.mlp.dense_h_to_4h.bias', 'lang_encoder.old_decoder_blocks.31.mlp.dense_4h_to_h.weight', 'lang_encoder.old_decoder_blocks.31.mlp.dense_4h_to_h.bias', 'lang_encoder.gated_cross_attn_layers.1.attn_gate', 'lang_encoder.gated_cross_attn_layers.1.ff_gate', 'lang_encoder.gated_cross_attn_layers.1.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.1.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.1.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.1.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.1.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.1.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.1.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.1.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.1.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.3.attn_gate', 'lang_encoder.gated_cross_attn_layers.3.ff_gate', 'lang_encoder.gated_cross_attn_layers.3.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.3.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.3.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.3.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.3.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.3.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.3.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.3.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.3.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.5.attn_gate', 'lang_encoder.gated_cross_attn_layers.5.ff_gate', 'lang_encoder.gated_cross_attn_layers.5.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.5.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.5.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.5.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.5.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.5.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.5.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.5.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.5.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.7.attn_gate', 'lang_encoder.gated_cross_attn_layers.7.ff_gate', 'lang_encoder.gated_cross_attn_layers.7.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.7.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.7.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.7.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.7.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.7.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.7.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.7.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.7.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.9.attn_gate', 'lang_encoder.gated_cross_attn_layers.9.ff_gate', 'lang_encoder.gated_cross_attn_layers.9.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.9.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.9.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.9.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.9.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.9.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.9.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.9.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.9.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.11.attn_gate', 'lang_encoder.gated_cross_attn_layers.11.ff_gate', 'lang_encoder.gated_cross_attn_layers.11.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.11.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.11.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.11.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.11.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.11.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.11.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.11.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.11.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.13.attn_gate', 'lang_encoder.gated_cross_attn_layers.13.ff_gate', 'lang_encoder.gated_cross_attn_layers.13.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.13.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.13.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.13.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.13.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.13.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.13.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.13.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.13.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.15.attn_gate', 'lang_encoder.gated_cross_attn_layers.15.ff_gate', 'lang_encoder.gated_cross_attn_layers.15.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.15.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.15.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.15.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.15.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.15.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.15.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.15.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.15.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.17.attn_gate', 'lang_encoder.gated_cross_attn_layers.17.ff_gate', 'lang_encoder.gated_cross_attn_layers.17.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.17.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.17.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.17.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.17.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.17.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.17.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.17.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.17.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.19.attn_gate', 'lang_encoder.gated_cross_attn_layers.19.ff_gate', 'lang_encoder.gated_cross_attn_layers.19.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.19.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.19.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.19.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.19.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.19.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.19.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.19.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.19.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.21.attn_gate', 'lang_encoder.gated_cross_attn_layers.21.ff_gate', 'lang_encoder.gated_cross_attn_layers.21.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.21.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.21.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.21.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.21.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.21.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.21.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.21.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.21.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.23.attn_gate', 'lang_encoder.gated_cross_attn_layers.23.ff_gate', 'lang_encoder.gated_cross_attn_layers.23.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.23.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.23.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.23.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.23.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.23.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.23.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.23.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.23.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.25.attn_gate', 'lang_encoder.gated_cross_attn_layers.25.ff_gate', 'lang_encoder.gated_cross_attn_layers.25.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.25.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.25.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.25.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.25.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.25.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.25.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.25.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.25.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.27.attn_gate', 'lang_encoder.gated_cross_attn_layers.27.ff_gate', 'lang_encoder.gated_cross_attn_layers.27.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.27.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.27.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.27.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.27.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.27.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.27.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.27.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.27.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.29.attn_gate', 'lang_encoder.gated_cross_attn_layers.29.ff_gate', 'lang_encoder.gated_cross_attn_layers.29.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.29.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.29.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.29.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.29.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.29.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.29.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.29.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.29.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.31.attn_gate', 'lang_encoder.gated_cross_attn_layers.31.ff_gate', 'lang_encoder.gated_cross_attn_layers.31.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.31.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.31.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.31.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.31.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.31.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.31.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.31.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.31.ff.3.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "\n",
    "model, image_processor, tokenizer = create_model_and_transforms(\n",
    "    clip_vision_encoder_path=\"ViT-L-14\",\n",
    "    clip_vision_encoder_pretrained=\"openai\",\n",
    "    lang_encoder_path=\"togethercomputer/RedPajama-INCITE-Instruct-3B-v1\",\n",
    "    tokenizer_path=\"togethercomputer/RedPajama-INCITE-Instruct-3B-v1\",\n",
    "    cross_attn_every_n_layers=2\n",
    ")\n",
    "\n",
    "# grab model checkpoint from huggingface hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-4B-vitl-rpj3b-langinstruct\", \"checkpoint.pt\")\n",
    "model.load_state_dict(torch.load(checkpoint_path), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b67d40f1-f958-415d-9019-3989b90b1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab model checkpoint from huggingface hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "device = \"cpu\"\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"CUDA is available! Using GPU for calculations.\")\n",
    "#     device = torch.device(\"cuda\")\n",
    "# else:\n",
    "#     print(\"CUDA is not available. Using CPU for calculations.\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e5d6c47-ff57-4a0b-8da7-4c3dac1cddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image>An image of two cats.<|endofchunk|><image>An image of a bathroom sink.<|endofchunk|><image>An image of a buffet table.<|endofchunk|>\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Step 1: Load images\n",
    "\"\"\"\n",
    "demo_image_one = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "demo_image_two = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028137.jpg\",\n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "query_image = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028352.jpg\", \n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 2: Preprocessing images\n",
    "Details: For OpenFlamingo, we expect the image to be a torch tensor of shape \n",
    " batch_size x num_media x num_frames x channels x height x width. \n",
    " In this case batch_size = 1, num_media = 3, num_frames = 1,\n",
    " channels = 3, height = 224, width = 224.\n",
    "\"\"\"\n",
    "vision_x = [image_processor(demo_image_one).unsqueeze(0), image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]\n",
    "vision_x = torch.cat(vision_x, dim=0)\n",
    "vision_x = vision_x.unsqueeze(1).unsqueeze(0).to(device)\n",
    "\n",
    "\"\"\"\n",
    "Step 3: Preprocessing text\n",
    "Details: In the text we expect an <image> special token to indicate where an image is.\n",
    " We also expect an <|endofchunk|> special token to indicate the end of the text \n",
    " portion associated with an image.\n",
    "\"\"\"\n",
    "tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "lang_x = tokenizer(\n",
    "    [\"<image>An image of two cats.<|endofchunk|><image>An image of a bathroom sink.<|endofchunk|><image>An image of\"],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 4: Generate text\n",
    "\"\"\"\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x,\n",
    "    lang_x=lang_x[\"input_ids\"],\n",
    "    attention_mask=lang_x[\"attention_mask\"],\n",
    "    max_new_tokens=20,\n",
    "    num_beams=3,\n",
    ")\n",
    "\n",
    "print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af882ee-1237-4c48-88c8-ab0a6b93801a",
   "metadata": {},
   "source": [
    "## 3.2 Few Shot Learning - only rgb input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2507ce1b-3283-4be6-9941-4553ccb50e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "Step 1: Load images\n",
    "\"\"\"\n",
    "dataset = GridDataset(grid_size=5, seed = 42, wall_symbol=\"#\", free_symbol=\".\", cell_size=5)\n",
    "img_rgb1, gridworld1 = dataset[0]\n",
    "img_rgb2, gridworld2 = dataset[1]\n",
    "img_rgb3, gridworld3 = dataset[2]\n",
    "img_rgb4, gridworld4 = dataset[3]\n",
    "ascii_inp1, path1 = str(gridworld1), gridworld1.a_star()\n",
    "ascii_inp2, path2 = str(gridworld2), gridworld2.a_star()\n",
    "ascii_inp3, path3 = str(gridworld3), gridworld3.a_star()\n",
    "ascii_inp4, path4 = str(gridworld4), gridworld4.a_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e03c4e9d-8395-4cbc-956f-3aa1806aba7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src_code.data_utils.dataset_utils import draw_image_grid\n",
    "draw_image_grid([(img_rgb1, path1), (img_rgb2, path2), (img_rgb3, path3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394cfda5-504d-44ec-836a-e8c45e88670a",
   "metadata": {},
   "source": [
    "## OpenFlamingo RGB Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1681bfe-2e60-4032-98d5-91d34761b18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image> In this example, the path from the starting cell to the goal cell is ('go up', 'go up', 'go up', 'go left')<|endofchunk|><image> is an image of a grid world\n",
      "\n",
      "The red cell is the starting cell,\n",
      "the green cell is the goal cell,\n",
      "the gray cells are obstacles,\n",
      "and the white cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up\n",
      "go down: move one cell down\n",
      "go left: move one cell left\n",
      "go right: move one cell right\n",
      "\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src_code.data_utils.prompt_utils import generate_inputs_for_openflamingo_rgb_and_text\n",
    "query_idx=1\n",
    "vision_x, lang_x, str_prompt = generate_inputs_for_openflamingo_rgb_and_text(tokenizer, image_processor, dataset, query_idx=query_idx, img_symbol=\"<image>\")\n",
    "print(str_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38bf5f7-693a-4b89-aae7-7caa85e89eb5",
   "metadata": {},
   "source": [
    "### 3.2.1 One Shot Learning RGB and Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aac9111e-ed42-4fd7-9cc3-7e4b40903173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image> In this example, the path from the starting cell to the goal cell is ('go up', 'go up', 'go up', 'go left')<|endofchunk|><image> is an image of a grid world\n",
      "\n",
      "The red cell is the starting cell,\n",
      "the green cell is the goal cell,\n",
      "the gray cells are obstacles,\n",
      "and the white cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell?<|endofchunk|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image> In this example, the path from the starting cell to the goal cell is ('go up', 'go up', 'go up', 'go left')<|endofchunk|><image> is an image of a grid world\n",
      "\n",
      "The red cell is the starting cell,\n",
      "the green cell is the goal cell,\n",
      "the gray cells are obstacles,\n",
      "and the white cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell?<|endofchunk|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image> In this example, the path from the starting cell to the goal cell is ('go up', 'go up', 'go up', 'go left')<|endofchunk|><image> is an image of a grid world\n",
      "\n",
      "The red cell is the starting cell,\n",
      "the green cell is the goal cell,\n",
      "the gray cells are obstacles,\n",
      "and the white cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell?<|endofchunk|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image> In this example, the path from the starting cell to the goal cell is ('go up', 'go up', 'go up', 'go left')<|endofchunk|><image> is an image of a grid world\n",
      "\n",
      "The red cell is the starting cell,\n",
      "the green cell is the goal cell,\n",
      "the gray cells are obstacles,\n",
      "and the white cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell?<|endofchunk|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image> In this example, the path from the starting cell to the goal cell is ('go up', 'go up', 'go up', 'go left')<|endofchunk|><image> is an image of a grid world\n",
      "\n",
      "The red cell is the starting cell,\n",
      "the green cell is the goal cell,\n",
      "the gray cells are obstacles,\n",
      "and the white cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell?<|endofchunk|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image> In this example, the path from the starting cell to the goal cell is ('go up', 'go up', 'go up', 'go left')<|endofchunk|><image> is an image of a grid world\n",
      "\n",
      "The red cell is the starting cell,\n",
      "the green cell is the goal cell,\n",
      "the gray cells are obstacles,\n",
      "and the white cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell?<|endofchunk|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image> In this example, the path from the starting cell to the goal cell is ('go up', 'go up', 'go up', 'go left')<|endofchunk|><image> is an image of a grid world\n",
      "\n",
      "The red cell is the starting cell,\n",
      "the green cell is the goal cell,\n",
      "the gray cells are obstacles,\n",
      "and the white cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell?<|endofchunk|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image> In this example, the path from the starting cell to the goal cell is ('go up', 'go up', 'go up', 'go left')<|endofchunk|><image> is an image of a grid world\n",
      "\n",
      "The red cell is the starting cell,\n",
      "the green cell is the goal cell,\n",
      "the gray cells are obstacles,\n",
      "and the white cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell?<|endofchunk|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image> In this example, the path from the starting cell to the goal cell is ('go up', 'go up', 'go up', 'go left')<|endofchunk|><image> is an image of a grid world\n",
      "\n",
      "The red cell is the starting cell,\n",
      "the green cell is the goal cell,\n",
      "the gray cells are obstacles,\n",
      "and the white cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell?<|endofchunk|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image> In this example, the path from the starting cell to the goal cell is ('go up', 'go up', 'go up', 'go left')<|endofchunk|><image> is an image of a grid world\n",
      "\n",
      "The red cell is the starting cell,\n",
      "the green cell is the goal cell,\n",
      "the gray cells are obstacles,\n",
      "and the white cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell?<|endofchunk|>\n"
     ]
    }
   ],
   "source": [
    "for query_idx in range(10):\n",
    "\n",
    "    vision_x, lang_x, str_prompt = generate_inputs_for_openflamingo_rgb_and_text(tokenizer, image_processor, dataset, img_symbol=\"<image>\", query_idx=query_idx)\n",
    "    \n",
    "    \"\"\"\n",
    "    Step 4: Generate text\n",
    "    \"\"\"\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x,\n",
    "        lang_x=lang_x[\"input_ids\"],\n",
    "        attention_mask=lang_x[\"attention_mask\"],\n",
    "        max_new_tokens=20,\n",
    "        num_beams=3,\n",
    "    )\n",
    "    \n",
    "    print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d1da3-38f3-4ca8-9296-1caf6e6116cb",
   "metadata": {},
   "source": [
    "### 3.2.2 only Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a017d47-270d-4f13-8c56-01cdb7913a9e",
   "metadata": {},
   "source": [
    "#### OpenFlamingo Only Text Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44380bd4-a353-4234-bebd-409b03917d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Coordinate system:\n",
      "The top-left cell is (0, 0).\n",
      "The y-coordinate increases to the right.\n",
      "The x-coordinate increases downwards.\n",
      "The starting cell is at (1, 4).\n",
      "The goal cell is at (0, 2).\n",
      "There are some obstacles at [(0, 0), (3, 1), (3, 3), (4, 0)].\n",
      "\n",
      "Output example:\n",
      "('go up', 'go right', 'go right', 'go down', 'go right')\n",
      "\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src_code.data_utils.prompt_utils import generate_inputs_for_openflamingo_text\n",
    "dummy_vision_x, lang_x, str_prompt = generate_inputs_for_openflamingo_text(tokenizer, image_processor, dataset, query_idx=1, pure_language=True)\n",
    "print(str_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00bb4d90-ffef-43af-8477-2865424ed9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  \n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Coordinate system:\n",
      "The top-left cell is (0, 0).\n",
      "The y-coordinate increases to the right.\n",
      "The x-coordinate increases downwards.\n",
      "The starting cell is at (3, 3).\n",
      "The goal cell is at (0, 2).\n",
      "There are some obstacles at [(1, 4), (2, 0), (2, 1), (3, 2), (3, 4), (4, 0)].\n",
      "\n",
      "Output example:\n",
      "('go up', 'go right', 'go right', 'go down', 'go right')\n",
      "\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell? Can you find the shortest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  \n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Coordinate system:\n",
      "The top-left cell is (0, 0).\n",
      "The y-coordinate increases to the right.\n",
      "The x-coordinate increases downwards.\n",
      "The starting cell is at (1, 4).\n",
      "The goal cell is at (0, 2).\n",
      "There are some obstacles at [(0, 0), (3, 1), (3, 3), (4, 0)].\n",
      "\n",
      "Output example:\n",
      "('go up', 'go right', 'go right', 'go down', 'go right')\n",
      "\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell? Can you find the shortest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  \n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Coordinate system:\n",
      "The top-left cell is (0, 0).\n",
      "The y-coordinate increases to the right.\n",
      "The x-coordinate increases downwards.\n",
      "The starting cell is at (0, 0).\n",
      "The goal cell is at (0, 2).\n",
      "There are some obstacles at [(1, 4), (3, 3), (3, 4)].\n",
      "\n",
      "Output example:\n",
      "('go up', 'go right', 'go right', 'go down', 'go right')\n",
      "\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell? Can you find the shortest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  \n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Coordinate system:\n",
      "The top-left cell is (0, 0).\n",
      "The y-coordinate increases to the right.\n",
      "The x-coordinate increases downwards.\n",
      "The starting cell is at (1, 4).\n",
      "The goal cell is at (4, 1).\n",
      "There are some obstacles at [(0, 3), (0, 4), (2, 4), (4, 0)].\n",
      "\n",
      "Output example:\n",
      "('go up', 'go right', 'go right', 'go down', 'go right')\n",
      "\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell? Can you find the shortest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  \n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Coordinate system:\n",
      "The top-left cell is (0, 0).\n",
      "The y-coordinate increases to the right.\n",
      "The x-coordinate increases downwards.\n",
      "The starting cell is at (1, 2).\n",
      "The goal cell is at (0, 3).\n",
      "There are some obstacles at [(0, 1), (0, 2), (1, 3), (3, 0), (3, 4)].\n",
      "\n",
      "Output example:\n",
      "('go up', 'go right', 'go right', 'go down', 'go right')\n",
      "\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell? Can you find the shortest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  \n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Coordinate system:\n",
      "The top-left cell is (0, 0).\n",
      "The y-coordinate increases to the right.\n",
      "The x-coordinate increases downwards.\n",
      "The starting cell is at (4, 2).\n",
      "The goal cell is at (2, 4).\n",
      "There are some obstacles at [(0, 0), (1, 0), (2, 0), (3, 1), (3, 3), (4, 0), (4, 1)].\n",
      "\n",
      "Output example:\n",
      "('go up', 'go right', 'go right', 'go down', 'go right')\n",
      "\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell? Can you find the shortest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  \n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Coordinate system:\n",
      "The top-left cell is (0, 0).\n",
      "The y-coordinate increases to the right.\n",
      "The x-coordinate increases downwards.\n",
      "The starting cell is at (4, 0).\n",
      "The goal cell is at (3, 2).\n",
      "There are some obstacles at [(0, 0), (0, 1), (1, 1), (1, 3), (2, 2), (3, 4)].\n",
      "\n",
      "Output example:\n",
      "('go up', 'go right', 'go right', 'go down', 'go right')\n",
      "\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell? Can you find the shortest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  \n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Coordinate system:\n",
      "The top-left cell is (0, 0).\n",
      "The y-coordinate increases to the right.\n",
      "The x-coordinate increases downwards.\n",
      "The starting cell is at (2, 1).\n",
      "The goal cell is at (3, 0).\n",
      "There are some obstacles at [(0, 0), (0, 3), (1, 0), (1, 2), (1, 3), (2, 2), (4, 0), (4, 3), (4, 4)].\n",
      "\n",
      "Output example:\n",
      "('go up', 'go right', 'go right', 'go down', 'go right')\n",
      "\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell? Can you find the shortest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  \n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Coordinate system:\n",
      "The top-left cell is (0, 0).\n",
      "The y-coordinate increases to the right.\n",
      "The x-coordinate increases downwards.\n",
      "The starting cell is at (1, 2).\n",
      "The goal cell is at (3, 1).\n",
      "There are some obstacles at [(0, 0), (0, 1), (0, 2), (1, 1), (4, 2)].\n",
      "\n",
      "Output example:\n",
      "('go up', 'go right', 'go right', 'go down', 'go right')\n",
      "\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell? Can you find the shortest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  \n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Coordinate system:\n",
      "The top-left cell is (0, 0).\n",
      "The y-coordinate increases to the right.\n",
      "The x-coordinate increases downwards.\n",
      "The starting cell is at (3, 4).\n",
      "The goal cell is at (2, 2).\n",
      "There are some obstacles at [(0, 0), (0, 2), (1, 0), (1, 3), (2, 4), (3, 1), (4, 4)].\n",
      "\n",
      "Output example:\n",
      "('go up', 'go right', 'go right', 'go down', 'go right')\n",
      "\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Can you find the shortest path from the starting cell to the goal cell? Can you find the shortest\n"
     ]
    }
   ],
   "source": [
    "for query_idx in range(10):\n",
    "    dummy_vision_x, lang_x, str_prompt = generate_inputs_for_openflamingo_text(tokenizer, image_processor, dataset, query_idx=query_idx, pure_language=True)\n",
    "    \"\"\"\n",
    "    Step 4: Generate text\n",
    "    \"\"\"\n",
    "    generated_text = model.generate(\n",
    "        vision_x=dummy_vision_x,\n",
    "        lang_x=lang_x[\"input_ids\"],\n",
    "        attention_mask=lang_x[\"attention_mask\"],\n",
    "        max_new_tokens=20,\n",
    "        num_beams=3,\n",
    "    )\n",
    "    \n",
    "    print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4689827b-39d6-42c4-8f9e-d7f1bbe9ac7b",
   "metadata": {},
   "source": [
    "### 3.2.3  only ASCII Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d134bfa-e62e-4436-a17b-6d700aab7a34",
   "metadata": {},
   "source": [
    "#### OpenFlamingo Only ASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2edc4b3-fab4-4614-97a3-ef93ea74deef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the grid world:\n",
      "# . G . . \n",
      ". . . . S \n",
      ". . . . . \n",
      ". # . # . \n",
      "# . . . . \n",
      "\n",
      "The S cell is the starting cell,\n",
      "the G cell is the goal cell,\n",
      "the # cells are obstacles,\n",
      "and the . cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up\n",
      "go down: move one cell down\n",
      "go left: move one cell left\n",
      "go right: move one cell right\n",
      "\n",
      "Output example:\n",
      "('go up', 'go right', 'go right', 'go down', 'go right')\n",
      "\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src_code.data_utils.prompt_utils import generate_inputs_for_openflamingo_text\n",
    "dummy_vision_x, lang_x, str_prompt = generate_inputs_for_openflamingo_text(tokenizer, image_processor, dataset, query_idx=1, pure_language=False)\n",
    "print(str_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02c5e010-0f19-4aa4-8cb8-3bd49d2e4a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      " Here is the grid world:\n",
      ".. G.. \n",
      ".... # \n",
      "# #... \n",
      ".. # S # \n",
      "#.... \n",
      "\n",
      "The S cell is the starting cell,\n",
      "the G cell is the goal cell,\n",
      "the # cells are obstacles,\n",
      "and the. cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Output example:\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Output example: Can you find the path from the starting cell to the goal cell? Here is the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      " Here is the grid world:\n",
      "#. G.. \n",
      ".... S \n",
      "..... \n",
      ". #. #. \n",
      "#.... \n",
      "\n",
      "The S cell is the starting cell,\n",
      "the G cell is the goal cell,\n",
      "the # cells are obstacles,\n",
      "and the. cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Output example:\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Answer: The path from the starting cell to the goal cell is: go up, go down,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      " Here is the grid world:\n",
      "S. G.. \n",
      ".... # \n",
      "..... \n",
      "... # # \n",
      "..... \n",
      "\n",
      "The S cell is the starting cell,\n",
      "the G cell is the goal cell,\n",
      "the # cells are obstacles,\n",
      "and the. cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Output example:\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Output example: Can you find the path from the starting cell to the goal cell? Here is the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      " Here is the grid world:\n",
      "... # # \n",
      ".... S \n",
      ".... # \n",
      "..... \n",
      "# G... \n",
      "\n",
      "The S cell is the starting cell,\n",
      "the G cell is the goal cell,\n",
      "the # cells are obstacles,\n",
      "and the. cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Output example:\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Output example: Can you find the shortest path from the starting cell to the goal cell? Output example\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      " Here is the grid world:\n",
      ". # # G. \n",
      ".. S #. \n",
      "..... \n",
      "#... # \n",
      "..... \n",
      "\n",
      "The S cell is the starting cell,\n",
      "the G cell is the goal cell,\n",
      "the # cells are obstacles,\n",
      "and the. cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Output example:\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Output example: Can you find the shortest path from the starting cell to the goal cell? Output example\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      " Here is the grid world:\n",
      "#.... \n",
      "#.... \n",
      "#... G \n",
      ". #. #. \n",
      "# # S.. \n",
      "\n",
      "The S cell is the starting cell,\n",
      "the G cell is the goal cell,\n",
      "the # cells are obstacles,\n",
      "and the. cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Output example:\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Output example: Can you find the shortest path from the starting cell to the goal cell? Output example\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      " Here is the grid world:\n",
      "# #... \n",
      ". #. #. \n",
      ".. #.. \n",
      ".. G. # \n",
      "S.... \n",
      "\n",
      "The S cell is the starting cell,\n",
      "the G cell is the goal cell,\n",
      "the # cells are obstacles,\n",
      "and the. cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Output example:\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Output example: Can you find the shortest path from the starting cell to the goal cell? Output example\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      " Here is the grid world:\n",
      "#.. #. \n",
      "#. # #. \n",
      ". S #.. \n",
      "G.... \n",
      "#.. # # \n",
      "\n",
      "The S cell is the starting cell,\n",
      "the G cell is the goal cell,\n",
      "the # cells are obstacles,\n",
      "and the. cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Output example:\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "Output example: Can you find the path from the starting cell to the goal cell? Here is the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      " Here is the grid world:\n",
      "# # #.. \n",
      ". # S.. \n",
      "..... \n",
      ". G... \n",
      ".. #.. \n",
      "\n",
      "The S cell is the starting cell,\n",
      "the G cell is the goal cell,\n",
      "the # cells are obstacles,\n",
      "and the. cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Output example:\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "The output should be a sequence of steps to reach the goal cell. Go up: move one cell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      " Here is the grid world:\n",
      "#. #.. \n",
      "#.. #. \n",
      ".. G. # \n",
      ". #.. S \n",
      ".... # \n",
      "\n",
      "The S cell is the starting cell,\n",
      "the G cell is the goal cell,\n",
      "the # cells are obstacles,\n",
      "and the. cells are free cells.\n",
      "\n",
      "Rules:\n",
      "The path must not pass through the obstacles.\n",
      "You can move up, down, left, or right from one cell to another.\n",
      "You cannot move diagonally.\n",
      "The path must be the shortest path from the starting cell to the goal cell.\n",
      "The output should be a sequence of steps to reach the goal cell.\n",
      "\n",
      "Actions:\n",
      "Only give me the steps, like 'go up', 'go down', 'go left' or 'go right'\n",
      "go up: move one cell up, in coordinate is x - 1\n",
      "go down: move one cell down, in coordinate is x + 1\n",
      "go left: move one cell left, in coordinate is y - 1\n",
      "go right: move one cell right, in coordinate is y + 1\n",
      "\n",
      "Output example:\n",
      "Can you find the path from the starting cell to the goal cell?\n",
      "The output should be a sequence of steps to reach the goal cell. Go up: move one cell\n"
     ]
    }
   ],
   "source": [
    "from src_code.data_utils.prompt_utils import generate_inputs_for_openflamingo_text\n",
    "\n",
    "\"\"\"\n",
    "Step 4: Generate text\n",
    "\"\"\"\n",
    "for query_idx in range(10):\n",
    "    dummy_vision_x, lang_x, str_prompt = generate_inputs_for_openflamingo_text(tokenizer, image_processor, dataset, query_idx=query_idx, pure_language=False)\n",
    "    generated_text = model.generate(\n",
    "        vision_x=dummy_vision_x,\n",
    "        lang_x=lang_x[\"input_ids\"],\n",
    "        attention_mask=lang_x[\"attention_mask\"],\n",
    "        max_new_tokens=20,\n",
    "        num_beams=3,\n",
    "    )\n",
    "    print(\"Generated text: \\n\", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb816a-50c5-454f-a2de-133678b83f99",
   "metadata": {},
   "source": [
    "# What we tried.\n",
    "1. directly prompt - garbage answer\n",
    "2. one shot propmt - ???\n",
    "3. two shot prompt - doesnt work\n",
    "4. different image resolutions\n",
    "5. ascii\n",
    "6. bigger models??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
