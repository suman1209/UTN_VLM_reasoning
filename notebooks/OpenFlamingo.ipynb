{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0833d1e6-e58c-4d66-bce9-1ad02b7ef476",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "## other standard packages\n",
    "import sys\n",
    "## Env variables and preparation stuffs\n",
    "sys.path.insert(0, \"../\")\n",
    "from src_code.data_utils.dataset import GridDataset\n",
    "from src_code.data_utils.dataset_utils import CellType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8137977c-ca7b-46a2-8e14-6dcab4274c35",
   "metadata": {},
   "source": [
    "# 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa20a2b-d8b0-40d1-a90a-bf42b52e1f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GridDataset(grid_size=5, seed = 42, wall_symbol=\"#\", free_symbol=\".\")\n",
    "\n",
    "img_rgb1, grid_world1 = dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b796b9-dce1-47a0-aa81-ad02f6633911",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rgb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48695ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(grid_world1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf9f9f1-1e1e-43ba-b9e9-f1995971804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{grid_world1.a_star() = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac2375-697b-44ea-8600-f22a8e981947",
   "metadata": {},
   "source": [
    "## 3 Open Flamingo Inference\n",
    "Source: https://github.com/mlfoundations/open_flamingo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28380ad-1873-4b86-8b53-bd02c3558769",
   "metadata": {},
   "source": [
    "## 3.1  Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d9b7d-3cbf-4cd3-8019-03f1039ea778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "\n",
    "model, image_processor, tokenizer = create_model_and_transforms(\n",
    "    clip_vision_encoder_path=\"ViT-L-14\",\n",
    "    clip_vision_encoder_pretrained=\"openai\",\n",
    "    lang_encoder_path=\"togethercomputer/RedPajama-INCITE-Instruct-3B-v1\",\n",
    "    tokenizer_path=\"togethercomputer/RedPajama-INCITE-Instruct-3B-v1\",\n",
    "    cross_attn_every_n_layers=2\n",
    ")\n",
    "\n",
    "# grab model checkpoint from huggingface hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-4B-vitl-rpj3b-langinstruct\", \"checkpoint.pt\")\n",
    "model.load_state_dict(torch.load(checkpoint_path), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d40f1-f958-415d-9019-3989b90b1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab model checkpoint from huggingface hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "device = \"cpu\"\n",
    "# if torch.cuda.is_available():\n",
    "#     print(\"CUDA is available! Using GPU for calculations.\")\n",
    "#     device = torch.device(\"cuda\")\n",
    "# else:\n",
    "#     print(\"CUDA is not available. Using CPU for calculations.\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5d6c47-ff57-4a0b-8da7-4c3dac1cddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Step 1: Load images\n",
    "\"\"\"\n",
    "demo_image_one = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "demo_image_two = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028137.jpg\",\n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "query_image = Image.open(\n",
    "    requests.get(\n",
    "        \"http://images.cocodataset.org/test-stuff2017/000000028352.jpg\", \n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 2: Preprocessing images\n",
    "Details: For OpenFlamingo, we expect the image to be a torch tensor of shape \n",
    " batch_size x num_media x num_frames x channels x height x width. \n",
    " In this case batch_size = 1, num_media = 3, num_frames = 1,\n",
    " channels = 3, height = 224, width = 224.\n",
    "\"\"\"\n",
    "vision_x = [image_processor(demo_image_one).unsqueeze(0), image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]\n",
    "vision_x = torch.cat(vision_x, dim=0)\n",
    "vision_x = vision_x.unsqueeze(1).unsqueeze(0).to(device)\n",
    "\n",
    "\"\"\"\n",
    "Step 3: Preprocessing text\n",
    "Details: In the text we expect an <image> special token to indicate where an image is.\n",
    " We also expect an <|endofchunk|> special token to indicate the end of the text \n",
    " portion associated with an image.\n",
    "\"\"\"\n",
    "tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "lang_x = tokenizer(\n",
    "    [\"<image>An image of two cats.<|endofchunk|><image>An image of a bathroom sink.<|endofchunk|><image>An image of\"],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 4: Generate text\n",
    "\"\"\"\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x,\n",
    "    lang_x=lang_x[\"input_ids\"],\n",
    "    attention_mask=lang_x[\"attention_mask\"],\n",
    "    max_new_tokens=20,\n",
    "    num_beams=3,\n",
    ")\n",
    "\n",
    "print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af882ee-1237-4c48-88c8-ab0a6b93801a",
   "metadata": {},
   "source": [
    "## 3.2 Few Shot Learning - only rgb input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507ce1b-3283-4be6-9941-4553ccb50e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "Step 1: Load images\n",
    "\"\"\"\n",
    "dataset = GridDataset(grid_size=5, seed = 42, wall_symbol=\"#\", free_symbol=\".\", cell_size=5)\n",
    "img_rgb1, gridworld1 = dataset[0]\n",
    "img_rgb2, gridworld2 = dataset[1]\n",
    "img_rgb3, gridworld3 = dataset[2]\n",
    "img_rgb4, gridworld4 = dataset[3]\n",
    "ascii_inp1, path1 = str(gridworld1), gridworld1.a_star()\n",
    "ascii_inp2, path2 = str(gridworld2), gridworld2.a_star()\n",
    "ascii_inp3, path3 = str(gridworld3), gridworld3.a_star()\n",
    "ascii_inp4, path4 = str(gridworld4), gridworld4.a_star()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c4e9d-8395-4cbc-956f-3aa1806aba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_code.data_utils.dataset_utils import draw_image_grid\n",
    "draw_image_grid([(img_rgb1, path1), (img_rgb2, path2), (img_rgb3, path3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394cfda5-504d-44ec-836a-e8c45e88670a",
   "metadata": {},
   "source": [
    "## OpenFlamingo RGB Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1681bfe-2e60-4032-98d5-91d34761b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_code.data_utils.prompt_utils import generate_inputs_for_openflamingo_rgb_and_text\n",
    "dataset = GridDataset(grid_size=5, seed = 42, wall_symbol=\"#\", free_symbol=\".\")\n",
    "query_idx=1\n",
    "vision_x, lang_x, str_prompt = generate_inputs_for_openflamingo_rgb_and_text(tokenizer, image_processor, dataset, query_idx=query_idx, img_symbol=\"<image>\")\n",
    "print(str_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ce374-b2be-4b8a-a502-f781dd4d11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_x[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c4dcc7-d6c9-4f50-a7c4-e5a57ab0139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(vision_x[0][1][0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d3ed6-a16b-4c25-9d02-0239f532bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38bf5f7-693a-4b89-aae7-7caa85e89eb5",
   "metadata": {},
   "source": [
    "### 3.2.1 One Shot Learning RGB and Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac9111e-ed42-4fd7-9cc3-7e4b40903173",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_idx in range(10):\n",
    "\n",
    "    vision_x, lang_x, str_prompt = generate_inputs_for_openflamingo_rgb_and_text(tokenizer, image_processor, dataset, img_symbol=\"<image>\", query_idx=query_idx)\n",
    "    \n",
    "    \"\"\"\n",
    "    Step 4: Generate text\n",
    "    \"\"\"\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x,\n",
    "        lang_x=lang_x[\"input_ids\"],\n",
    "        attention_mask=lang_x[\"attention_mask\"],\n",
    "        max_new_tokens=20,\n",
    "        num_beams=3,\n",
    "    )\n",
    "    \n",
    "    print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d1da3-38f3-4ca8-9296-1caf6e6116cb",
   "metadata": {},
   "source": [
    "### 3.2.2 only Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a017d47-270d-4f13-8c56-01cdb7913a9e",
   "metadata": {},
   "source": [
    "#### OpenFlamingo Only Text Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44380bd4-a353-4234-bebd-409b03917d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_code.data_utils.prompt_utils import generate_inputs_for_openflamingo_text\n",
    "dummy_vision_x, lang_x, str_prompt = generate_inputs_for_openflamingo_text(tokenizer, image_processor, dataset, query_idx=1, pure_language=True)\n",
    "print(str_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bb4d90-ffef-43af-8477-2865424ed9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query_idx in range(1):\n",
    "    dummy_vision_x, lang_x, str_prompt = generate_inputs_for_openflamingo_text(tokenizer, image_processor, dataset, query_idx=query_idx, pure_language=True)\n",
    "    \"\"\"\n",
    "    Step 4: Generate text\n",
    "    \"\"\"\n",
    "    generated_text = model.generate(\n",
    "        vision_x=dummy_vision_x,\n",
    "        lang_x=lang_x[\"input_ids\"],\n",
    "        attention_mask=lang_x[\"attention_mask\"],\n",
    "        max_new_tokens=20,\n",
    "        num_beams=3,\n",
    "    )\n",
    "    \n",
    "    print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4689827b-39d6-42c4-8f9e-d7f1bbe9ac7b",
   "metadata": {},
   "source": [
    "### 3.2.3  only ASCII Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d134bfa-e62e-4436-a17b-6d700aab7a34",
   "metadata": {},
   "source": [
    "#### OpenFlamingo Only ASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2edc4b3-fab4-4614-97a3-ef93ea74deef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src_code.data_utils.prompt_utils import generate_inputs_for_openflamingo_text\n",
    "dummy_vision_x, lang_x, str_prompt = generate_inputs_for_openflamingo_text(tokenizer, image_processor, dataset, query_idx=1, pure_language=False)\n",
    "print(str_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c5e010-0f19-4aa4-8cb8-3bd49d2e4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_code.data_utils.prompt_utils import generate_inputs_for_openflamingo_text\n",
    "\n",
    "\"\"\"\n",
    "Step 4: Generate text\n",
    "\"\"\"\n",
    "for query_idx in range(1):\n",
    "    dummy_vision_x, lang_x, str_prompt = generate_inputs_for_openflamingo_text(tokenizer, image_processor, dataset, query_idx=query_idx, pure_language=False)\n",
    "    generated_text = model.generate(\n",
    "        vision_x=dummy_vision_x,\n",
    "        lang_x=lang_x[\"input_ids\"],\n",
    "        attention_mask=lang_x[\"attention_mask\"],\n",
    "        max_new_tokens=20,\n",
    "        num_beams=3,\n",
    "    )\n",
    "    print(\"Generated text: \\n\", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb816a-50c5-454f-a2de-133678b83f99",
   "metadata": {},
   "source": [
    "# What we tried.\n",
    "1. directly prompt - garbage answer\n",
    "2. one shot propmt - ???\n",
    "3. two shot prompt - doesnt work\n",
    "4. different image resolutions\n",
    "5. ascii\n",
    "6. bigger models??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
